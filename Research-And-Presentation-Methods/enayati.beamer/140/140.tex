\documentclass[a4,9pt]{beamer}
\usetheme{Berlin}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{graphicx}
\linespread{1.35}
\usepackage{amsmath}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\begin{document}

\begin{frame}
\section*{Minimization of Finite Automata}
\begin{flushright}
 \texttt{CHAPTER TEN} \hspace*{0.1cm}\textbf{$|$} \hspace*{0.1cm} \textbf{140}\hspace*{0.1cm}
\end{flushright}
\vspace*{1cm}

the Net-based environments. This section discusses means and ways to mine data from Net-based environments.
All activity that takes place on a Web site or in a virtual environment is normally logged or stored by the program owner. However, the difference between the promise of accurate and meaningful information and the reality of what one finds in the thousands of lines of raw data produced by a Web server log has inspired a host of Web analysis applications and even more customized solutions. These applications produce a wide variety of individual and summary data, some of which may be useful to the e-researcher. If, for example, one is studying the use of a dedicated educational suite of Web-enabled software, such as First Class, WebCT, or BlackBoard, the program may itself be gathering and presenting data on user activity. This is almost always the easiest data to access.
\end{frame}
\begin{document}

\begin{frame}
\section*{Minimization of Finite Automata}
\begin{flushright}
 \texttt{CHAPTER TEN} \hspace*{0.1cm}\textbf{$|$} \hspace*{0.1cm} \textbf{140}\hspace*{0.1cm}
\end{flushright}
\vspace*{1cm}

However, most of the data collected by these programs is designed for educational purposes, rather than research purposes and, hence, may not be optimized for e-research. As such, the e-researcher may need to turn to one of the
more sophisticated commercial or freeware tools designed to assist researchers in identifying and measuring the activities that users engage in while visiting a site. Unfortunately, most of these tools are focused clearly on the e-commerce market. The tools available, while interesting and of potential value for some types of e-research, may be too focused on analysis of behavior that is directly related to current or future sales prospects. Concurrently the prices of some of these products reflect their commercial orientation.
Aberdeen Consulting coined the term insight-to-effort ratio in regard to Web analyzer software to highlight the amount of effort required of the e-researcher to extract meaningful insights from the behavior of users of the site. Complex analysis information may be very time consuming and require special programming and data extraction skills.

\end{frame}
\begin{document}

\begin{frame}
\section*{Minimization of Finite Automata}
\begin{flushright}
 \texttt{CHAPTER TEN} \hspace*{0.1cm}\textbf{$|$} \hspace*{0.1cm} \textbf{140}\hspace*{0.1cm}
\end{flushright}
\vspace*{1cm}

At the lowest level, a researcher can use simple text analysis tools to examine and extract data from the Web logs themselves. However these logs are usually overly detailed and not formatted for ease of understanding or analysis. In all likelihood, work spent analyzing raw server logs produces a very low insight-to-effort ratio.
Alternatively, an e-researcher may choose to lease or buy a high-end Web analyzer package and achieve insights at low effort, though probably at a high price. The e-researcher's task, then, is to select a set of tools that provides a high insight-to-effort ratio without exceeding our often limited research budgets.
The basic features of Web analysis programs relevant to researchers include:
■ Number of hits at specific pages.
■ Amount of time between hits, thus indicating the time visitors spend at each page.
■ Reviews of the path followed by subjects through particular educational sites.
\end{frame}
\end{document} 